---
---

@article{barquero2021rank,
  bibtex_show={true},
  abbr={T-BIOM},
  title={Rank-based verification for long-term face tracking in crowded scenes},
  author={Barquero, German and Hupont, Isabelle and Tena, Carles Fernandez},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
  volume={3},
  number={4},
  pages={495--505},
  year={2021},
  publisher={IEEE},
  code={https://github.com/hertasecurity/LTFT},
  html={https://ieeexplore.ieee.org/abstract/document/9494449},
  arxiv={2107.13273},
  abstract={Most current multi-object trackers focus on short-term tracking, and are based on deep and complex systems that often cannot operate in real-time, making them impractical for video-surveillance. In this paper we present a long-term, multi-face tracking architecture conceived for working in crowded contexts where faces are often the only visible part of a person. Our system benefits from advances in the fields of face detection and face recognition to achieve long-term tracking, and is particularly unconstrained to the motion and occlusions of people. It follows a tracking-by-detection approach, combining a fast short-term visual tracker with a novel online tracklet reconnection strategy grounded on rank-based face verification. The proposed rank-based constraint favours higher inter-class distance among tracklets, and reduces the propagation of errors due to wrong reconnections. Additionally, a correction module is included to correct past assignments with no extra computational cost. We present a series of experiments introducing novel specialized metrics for the evaluation of long-term tracking capabilities, and publicly release a video dataset with 10 manually annotated videos and a total length of 8' 54". Our findings validate the robustness of each of the proposed modules, and demonstrate that, in these challenging contexts, our approach yields up to 50% longer tracks than state-of-the-art deep learning trackers.}
}

@inproceedings{barquero2020long,
  bibtex_show={true},
  abbr={IJCB},
  title={Long-term face tracking for crowded video-surveillance scenarios},
  author={Barquero, German and Fernandez, Carles and Hupont, Isabelle},
  booktitle={2020 IEEE International Joint Conference on Biometrics},
  pages={1--8},
  organization={IEEE},
  code={https://github.com/hertasecurity/LTFT},
  html={https://ieeexplore.ieee.org/abstract/document/9304892},
  arxiv={2010.08675},
  abstract={Most current multi-object trackers focus on short-term tracking, and are based on deep and complex systems that do not operate in real-time, often making them impractical for video-surveillance. In this paper, we present a long-term multi-face tracking architecture conceived for working in crowded contexts, particularly unconstrained in terms of movement and occlusions, and where the face is often the only visible part of the person. Our system benefits from advances in the fields of face detection and face recognition to achieve long-term tracking. It follows a tracking-by-detection approach, combining a fast short-term visual tracker with a novel online tracklet reconnection strategy grounded on face verification. Additionally, a correction module is included to correct past track assignments with no extra computational cost. We present a series of experiments introducing novel, specialized metrics for the evaluation of long-term tracking capabilities and a video dataset that we publicly release. Findings demonstrate that, in this context, our approach allows to obtain up to 50% longer tracks than state-of-the-art deep learning trackers.}
}

@article{la2021mprage,
  bibtex_show={true},
  abbr={Comp. BioMed},
  title={MPRAGE to MP2RAGE UNI translation via generative adversarial network improves the automatic tissue and lesion segmentation in multiple sclerosis patients},
  author={La Rosa, Francesco and Yu, Thomas and Barquero, German and Thiran, Jean-Philippe and Granziera, Cristina and Cuadra, Meritxell Bach},
  journal={Computers in biology and medicine},
  volume={132},
  pages={104297},
  year={2021},
  publisher={Elsevier},
  html={https://www.sciencedirect.com/science/article/pii/S0010482521000913},
  pdf={2021_mp2rage_gan.pdf},
  abstract={
    <b>Background and objective</b><br>
    Compared to the conventional magnetization-prepared rapid gradient-echo imaging (MPRAGE) MRI sequence, the specialized magnetization prepared 2 rapid acquisition gradient echoes (MP2RAGE) shows a higher brain tissue and lesion contrast in multiple sclerosis (MS) patients. The goal of this work is to retrospectively generate realistic-looking MP2RAGE uniform images (UNI) from already acquired MPRAGE images in order to improve the automatic lesion and tissue segmentation.
    <b>Methods</b><br>
    For this task we propose a generative adversarial network (GAN). Multi-contrast MRI data of 12 healthy controls and 44 patients diagnosed with MS was retrospectively analyzed. Imaging was acquired at 3T using a SIEMENS scanner with MPRAGE, MP2RAGE, FLAIR, and DIR sequences. We train the GAN with both healthy controls and MS patients to generate synthetic MP2RAGE UNI images. These images were then compared to the real MP2RAGE UNI (considered as ground truth) analyzing the output of automatic brain tissue and lesion segmentation tools. Reference-based metrics as well as the lesion-wise true and false positives, Dice coefficient, and volume difference were considered for the evaluation. Statistical differences were assessed with the Wilcoxon signed-rank test.
    <b>Results</b><br>
    The synthetic MP2RAGE UNI significantly improves the lesion and tissue segmentation masks in terms of Dice coefficient and volume difference (p-values < 0.001) compared to the MPRAGE. For the segmentation metrics analyzed no statistically significant differences are found between the synthetic and acquired MP2RAGE UNI.
    <b>Conclusion</b><br>
    Synthesized MP2RAGE UNI images are visually realistic and improve the output of automatic segmentation tools.
  }
}

@article{barquero2020rimnet,
  bibtex_show={true},
  abbr={NeuroImage},
  title={RimNet: A deep 3D multimodal MRI architecture for paramagnetic rim lesion assessment in multiple sclerosis},
  author={Germán Barquero and Francesco {La Rosa} and Hamza Kebiri and Po-Jui Lu and Reza Rahmanzadeh and Matthias Weigel and Mário João Fartaria and Tobias Kober and Marie Théaudin and Renaud {Du Pasquier} and Pascal Sati and Daniel S. Reich and Martina Absinta and Cristina Granziera and Pietro Maggi and Meritxell {Bach Cuadra}},
  journal={NeuroImage: Clinical},
  volume={28},
  pages={102412},
  year={2020},
  publisher={Elsevier},
  html={https://www.sciencedirect.com/science/article/pii/S2213158220302497},
  pdf={2020_rimnet.pdf},
  abstract={<b>Objectives</b><br>
    In multiple sclerosis (MS), the presence of a paramagnetic rim at the edge of non-gadolinium-enhancing lesions indicates perilesional chronic inflammation. Patients featuring a higher paramagnetic rim lesion burden tend to have more aggressive disease. The objective of this study was to develop and evaluate a convolutional neural network (CNN) architecture (RimNet) for automated detection of paramagnetic rim lesions in MS employing multiple magnetic resonance (MR) imaging contrasts.
    <b>Materials and methods</b><br>
    Imaging data were acquired at 3 Tesla on three different scanners from two different centers, totaling 124 MS patients, and studied retrospectively. Paramagnetic rim lesion detection was independently assessed by two expert raters on T2*-phase images, yielding 462 rim-positive (rim+) and 4857 rim-negative (rim-) lesions. RimNet was designed using 3D patches centered on candidate lesions in 3D-EPI phase and 3D FLAIR as input to two network branches. The interconnection of branches at both the first network blocks and the last fully connected layers favors the extraction of low and high-level multimodal features, respectively. RimNet’s performance was quantitatively evaluated against experts’ evaluation from both lesion-wise and patient-wise perspectives. For the latter, patients were categorized based on a clinically relevant threshold of 4 rim+ lesions per patient. The individual prediction capabilities of the images were also explored and compared (DeLong test) by testing a CNN trained with one image as input (unimodal).
    <b>Results</b><br>
    The unimodal exploration showed the superior performance of 3D-EPI phase and 3D-EPI magnitude images in the rim+/- classification task (AUC = 0.913 and 0.901), compared to the 3D FLAIR (AUC = 0.855, Ps < 0.0001). The proposed multimodal RimNet prototype clearly outperformed the best unimodal approach (AUC = 0.943, P < 0.0001). The sensitivity and specificity achieved by RimNet (70.6% and 94.9%, respectively) are comparable to those of experts at the lesion level. In the patient-wise analysis, RimNet performed with an accuracy of 89.5% and a Dice coefficient (or F1 score) of 83.5%.
    <b>Conclusions</b><br>
    The proposed prototype showed promising performance, supporting the usage of RimNet for speeding up and standardizing the paramagnetic rim lesions analysis in MS.
    }
}